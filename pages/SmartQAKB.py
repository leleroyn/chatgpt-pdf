import os
from io import BytesIO
from time import time

import streamlit as st
from PIL import Image

from service.OcrService import OcrService
from service.VectorDBService import VectorDBService
from service.vllm_inference import text_inference_with_llm


class SmartQAKB:
    def __init__(self):
        self.ocr_service = OcrService()
        self.vector_db_service = VectorDBService()
        self.kb_name = "Simple_KB"
    
    def initialize_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        try:
            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            collections = self.vector_db_service.qdrant_client.get_collections().collections
            collection_names = [collection.name for collection in collections]
            
            if self.kb_name in collection_names:
                # å°è¯•æœç´¢æ¥æ£€æŸ¥é›†åˆæ˜¯å¦æœ‰å†…å®¹
                try:
                    search_results = self.vector_db_service.search_similar_chunks("test", top_k=1)
                    if search_results:
                        st.success(f"çŸ¥è¯†åº“ '{self.kb_name}' å·²åŠ è½½ï¼ŒåŒ…å«å†…å®¹")
                    else:
                        st.info(f"çŸ¥è¯†åº“ '{self.kb_name}' å·²åŠ è½½ï¼ˆå½“å‰ä¸ºç©ºï¼Œè¯·ä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“ï¼‰")
                except Exception:
                    st.info(f"çŸ¥è¯†åº“ '{self.kb_name}' å·²åŠ è½½ï¼ˆå½“å‰ä¸ºç©ºï¼Œè¯·ä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“ï¼‰")
            else:
                # çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“
                st.info(f"çŸ¥è¯†åº“ '{self.kb_name}' ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çŸ¥è¯†åº“")
                self.vector_db_service._ensure_collection_exists()
                st.success(f"çŸ¥è¯†åº“ '{self.kb_name}' åˆ›å»ºæˆåŠŸï¼ˆå½“å‰ä¸ºç©ºï¼‰")
                
            return True
        except Exception as e:
            st.error(f"åˆå§‹åŒ–çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            return False
    
    def clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            collections = self.vector_db_service.qdrant_client.get_collections().collections
            collection_names = [collection.name for collection in collections]
            
            if self.kb_name in collection_names:
                self.vector_db_service.qdrant_client.delete_collection(self.kb_name)
                st.success(f"çŸ¥è¯†åº“ '{self.kb_name}' å·²æ¸…ç©º")
            else:
                st.info(f"çŸ¥è¯†åº“ '{self.kb_name}' ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©º")
            
            return True
        except Exception as e:
            st.warning(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            return True
    
    def clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è¯†åº“å†…å®¹"""
        try:
            collections = self.vector_db_service.qdrant_client.get_collections().collections
            collection_names = [collection.name for collection in collections]
            
            if self.kb_name in collection_names:
                # åˆ é™¤é›†åˆå†…æ‰€æœ‰ç‚¹
                from qdrant_client.models import Filter
                self.vector_db_service.qdrant_client.delete(
                    collection_name=self.kb_name,
                    points_selector=Filter()
                )
                st.success(f"çŸ¥è¯†åº“ '{self.kb_name}' å†…å®¹å·²æ¸…ç©º")
                return True
            else:
                st.info(f"çŸ¥è¯†åº“ '{self.kb_name}' ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©º")
                return False
                
        except Exception as e:
            st.error(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            return False
    
    def process_uploaded_file(self, uploaded_file):
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œè¿›è¡ŒOCRè¯†åˆ«"""
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        if file_extension in ['jpg', 'jpeg', 'png', 'bmp']:
            # å¤„ç†å›¾ç‰‡æ–‡ä»¶
            image = uploaded_file.getvalue()
            pil_image = Image.open(BytesIO(image))
            ocr_result = self.ocr_service.detect_from_image(pil_image)
            return " ".join(ocr_result) if ocr_result else ""
            
        elif file_extension == 'pdf':
            # å¤„ç†PDFæ–‡ä»¶
            pdf_bytes = uploaded_file.getvalue()
            pdf_text_dict = self.ocr_service.detect_from_pdf_path(pdf_bytes)
            
            # åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
            all_text = []
            for page_num, (img, text_list) in pdf_text_dict.items():
                all_text.extend(text_list)
            
            return " ".join(all_text) if all_text else ""
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
    
    def chunk_text(self, text, chunk_size=500, overlap=50):
        """å°†æ–‡æœ¬åˆ†å—"""
        return self.vector_db_service.chunk_text(text, chunk_size, overlap)
    
    def store_in_knowledge_base(self, text_chunks, document_id):
        """å°†æ–‡æœ¬å—å­˜å‚¨åˆ°çŸ¥è¯†åº“"""
        return self.vector_db_service.store_document_chunks(document_id, text_chunks)
    
    def search_knowledge_base(self, query, top_k=5):
        """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸ä¼¼æ–‡æœ¬å—"""
        return self.vector_db_service.search_similar_chunks(query, top_k)


def main():
    # åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv()
    
    # é¡µé¢é…ç½®
    st.set_page_config(page_title="æ™ºèƒ½é—®ç­”(Simple_KB)", layout="wide", menu_items={})
    st.subheader(f"ğŸ¤– æ™ºèƒ½é—®ç­”(Simple_KB)")
    
    # æ¸…é™¤æ‰€æœ‰ä¼šè¯çŠ¶æ€ï¼Œç¡®ä¿æ¯æ¬¡æ‰“å¼€éƒ½æ˜¯æ–°çš„
    st.session_state.clear()
    
    # åˆå§‹åŒ–åº”ç”¨
    st.session_state.qa_system = SmartQAKB()
    qa_system = st.session_state.qa_system
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    if not qa_system.initialize_knowledge_base():
        st.error("çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æ“ä½œ")
        return
    
    # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦æœ‰å†…å®¹
    has_content = False
    try:
        search_results = qa_system.search_knowledge_base("test", top_k=1)
        has_content = len(search_results) > 0
    except:
        has_content = False
    
    # åˆå§‹åŒ–ä¸Šä¼ æ–‡ä»¶å˜é‡åˆ°ä¼šè¯çŠ¶æ€
    if 'uploaded_file' not in st.session_state:
        st.session_state.uploaded_file = None
    
    # åˆ›å»ºä¸‰åˆ—å¸ƒå±€
    col1, col2, col3 = st.columns(3, gap="medium")
    
    # å·¦ä¾§åˆ—ï¼šé—®ç­”åŒºåŸŸ
    with col1:
        st.divider()
        
        # å¦‚æœæœ‰å†…å®¹ä½†æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼Œæ˜¾ç¤ºç›´æ¥æé—®åŒºåŸŸ
        if has_content and st.session_state.uploaded_file is None:
            st.success("ğŸ’¡ çŸ¥è¯†åº“å·²æœ‰å†…å®¹ï¼Œæ‚¨å¯ä»¥ç›´æ¥æé—®")
        
        # æ˜¾ç¤ºé—®ç­”åŒºåŸŸ
        user_input = st.text_area(
            label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", 
            placeholder="æ€»ç»“çŸ¥è¯†åº“çš„ä¸»è¦å†…å®¹.",           
            height=100
        )
        
        # æ™ºèƒ½é—®ç­”æŒ‰é’®
        if st.button("æ™ºèƒ½é—®ç­”", type="secondary"):
            if not user_input.strip():
                st.error("è¯·è¾“å…¥é—®é¢˜å†…å®¹")
                return
            
            with st.spinner("æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“..."):
                try:
                    search_results = qa_system.search_knowledge_base(user_input, top_k=6)
                    st.session_state.search_results = search_results
                    st.session_state.user_query = user_input
                    st.success(f"æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æœ¬å—")
                    
                except Exception as e:
                    st.error(f"æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
        
        # çŸ¥è¯†åº“ç®¡ç†æŒ‰é’® - æ”¾åœ¨æ™ºèƒ½é—®ç­”æŒ‰é’®åé¢ï¼Œé¿å…è¯¯ç‚¹
        st.divider()
        with st.expander("çŸ¥è¯†åº“ç®¡ç†", expanded=False):
            # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ - æ•´åˆåˆ°çŸ¥è¯†åº“ç®¡ç†
            uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£æ–‡ä»¶", type=["jpg", "jpeg", "png", "bmp", "pdf"])
            
            if uploaded_file:
                st.session_state.uploaded_file = uploaded_file
                st.success(f"å·²ä¸Šä¼ : {uploaded_file.name}")
            
            # åˆ›å»ºä¸¤åˆ—å¸ƒå±€æ”¾ç½®çŸ¥è¯†åº“ç®¡ç†æŒ‰é’®
            col_update, col_delete = st.columns(2)
            
            with col_update:
                # æ›´æ–°çŸ¥è¯†åº“æŒ‰é’®
                if st.session_state.uploaded_file:
                    if st.button("æ›´æ–°çŸ¥è¯†åº“", type="primary", help="å‘çŸ¥è¯†åº“æ·»åŠ æ–°æ–‡æ¡£ï¼Œä¸æ¸…ç©ºç°æœ‰å†…å®¹"):
                        with st.spinner("æ­£åœ¨æ›´æ–°çŸ¥è¯†åº“..."):
                            try:
                                # OCRè¯†åˆ«
                                ocr_text = qa_system.process_uploaded_file(st.session_state.uploaded_file)
                                
                                if not ocr_text.strip():
                                    st.error("OCRè¯†åˆ«å¤±è´¥æˆ–æœªè¯†åˆ«åˆ°æ–‡æœ¬å†…å®¹")
                                    return
                                
                                st.success(f"OCRè¯†åˆ«å®Œæˆï¼Œè¯†åˆ«åˆ° {len(ocr_text)} ä¸ªå­—ç¬¦")
                                
                                # æ–‡æœ¬åˆ†å—
                                text_chunks = qa_system.chunk_text(ocr_text)
                                st.success(f"æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…± {len(text_chunks)} ä¸ªå—")
                                
                                # å­˜å‚¨åˆ°çŸ¥è¯†åº“
                                document_id = f"doc_{int(time())}"
                                stored_count = qa_system.store_in_knowledge_base(text_chunks, document_id)
                                st.success(f"çŸ¥è¯†åº“æ›´æ–°å®Œæˆï¼Œæ–°å¢äº† {stored_count} ä¸ªæ–‡æœ¬å—")
                                
                                # ä¿å­˜å¤„ç†çŠ¶æ€
                                st.session_state.knowledge_base = {
                                    'document_id': document_id,
                                    'ocr_text': ocr_text,
                                    'chunk_count': len(text_chunks),
                                    'kb_name': qa_system.kb_name
                                }
                                
                                st.experimental_rerun()
                                
                            except Exception as e:
                                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            
            with col_delete:
                # æ¸…ç©ºçŸ¥è¯†åº“æŒ‰é’®
                if st.button("æ¸…ç©ºçŸ¥è¯†åº“", type="secondary", help="æ¸…ç©ºçŸ¥è¯†åº“æ‰€æœ‰å†…å®¹ä½†ä¿ç•™é›†åˆç»“æ„"):
                    with st.spinner("æ­£åœ¨æ¸…ç©ºçŸ¥è¯†åº“..."):
                        try:
                            if qa_system.clear_knowledge_base():
                                st.session_state.clear()
                                st.session_state.qa_system = SmartQAKB()  # é‡æ–°åˆå§‹åŒ–
                                st.experimental_rerun()
                            else:
                                st.error("æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥")
                        except Exception as e:
                            st.error(f"æ¸…ç©ºæ“ä½œå‡ºé”™: {str(e)}")
    
    # ä¸­é—´åˆ—ï¼šæœç´¢ç»“æœ
    with col2:
        st.divider()
        
        # æ˜¾ç¤ºOCRåŸå§‹æ–‡æœ¬ï¼ˆå¦‚æœæœ‰æ–°ä¸Šä¼ çš„æ–‡æ¡£ï¼‰
        if 'knowledge_base' in st.session_state:
            st.success(f"å·²å¤„ç†æ–‡æ¡£: {len(st.session_state.knowledge_base['ocr_text'])} å­—ç¬¦, {st.session_state.knowledge_base['chunk_count']} ä¸ªæ–‡æœ¬å—")
        
        # æ˜¾ç¤ºæœç´¢ç»“æœ
        if 'search_results' in st.session_state:
            st.info("æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯")
            for i, result in enumerate(st.session_state.search_results):
                with st.expander(f"ç›¸å…³æ–‡æœ¬å— {i+1} (ç›¸ä¼¼åº¦: {result['score']:.3f})"):
                    st.text_area(
                        label=f"æ–‡æœ¬å†…å®¹ {i+1}",
                        value=result['text'],
                        height=150,
                        disabled=True
                    )
    
    # å³ä¾§åˆ—ï¼šå¤§æ¨¡å‹å›ç­”
    with col3:
        st.divider()
        
        # æ˜¾ç¤ºå¤§æ¨¡å‹å›ç­”
        if 'search_results' in st.session_state:
            st.info("æ™ºèƒ½åˆ†æç»“æœ")
            
            # æ„å»ºæç¤ºè¯
            query = st.session_state.user_query
            context_parts = [result['text'] for result in st.session_state.search_results]
            context = "\n\n".join(context_parts)
            
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
            è¦æ±‚ï¼š
            1. åªåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
            2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€ä¸“ä¸š"""
            
            prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®æå–ç›¸å…³ä¿¡æ¯å¹¶ç»™å‡ºå›ç­”ã€‚"""
            
            # é›†æˆå¤§æ¨¡å‹API
            try:
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ™ºèƒ½åˆ†æ..."):
                    llm_response = text_inference_with_llm(
                        prompt=prompt,
                        system_message=system_prompt,
                        temperature=0.7,
                        max_tokens=1000
                    )
                    
                    if llm_response:
                        st.markdown(llm_response)
                    else:
                        st.warning("å¤§æ¨¡å‹æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
                        
            except Exception as e:
                st.error(f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
                st.text_area(
                    label="åˆ†æç»“æœ",
                    value="å¤§æ¨¡å‹æœåŠ¡è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡é…ç½®ã€‚",
                    height=200,
                    disabled=True
                )
            
            # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
            if 'knowledge_base' in st.session_state:
                st.info("çŸ¥è¯†åº“ç»Ÿè®¡")
                st.write(f"- çŸ¥è¯†åº“åç§°: {st.session_state.knowledge_base['kb_name']}")
                st.write(f"- æ–‡æ¡£ID: {st.session_state.knowledge_base['document_id']}")
                st.write(f"- æ–‡æœ¬å­—ç¬¦æ•°: {len(st.session_state.knowledge_base['ocr_text'])}")
                st.write(f"- æ–‡æœ¬å—æ•°é‡: {st.session_state.knowledge_base['chunk_count']}")


if __name__ == '__main__':
    main()